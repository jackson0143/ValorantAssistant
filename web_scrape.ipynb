{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Eunha\\Documents\\Personal work\\ValorantAssistant\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q requests beautifulsoup4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://valorant.fandom.com\"\n",
    "AGENTS_PAGE = \"https://valorant.fandom.com/wiki/Agents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_links():\n",
    "    res = requests.get(AGENTS_PAGE)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    agent_links = []\n",
    "    # Find the main agents table\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "    if not table:\n",
    "        print(\"Could not find agents table\")\n",
    "        return []\n",
    "\n",
    "    rows = table.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        # Get the second cell which has agent link\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) > 1:\n",
    "            cell = cells[1]\n",
    "            link = cell.find(\"a\")\n",
    "            if link and link.get(\"href\") and \"/wiki/\" in link.get(\"href\"):\n",
    "                agent_links.append(BASE_URL + link.get(\"href\"))\n",
    "    return list(set(agent_links))\n",
    "\n",
    "\n",
    "def parse_agent_page(url):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        # Get basic info\n",
    "        name = soup.find(\"h1\", id=\"firstHeading\").text.strip()\n",
    "\n",
    "\n",
    "    \n",
    "        info_box = soup.find_all('section', class_='pi-item pi-group pi-border-color pi-collapse pi-collapse-open')\n",
    "        bio_box = info_box[0]\n",
    "        game_details_box= info_box[1]\n",
    "        bio_info = {}\n",
    "        if bio_box:\n",
    "            bio_field_mapping = {\n",
    "                'realname': 'real_name',\n",
    "                'aliases': 'aliases',\n",
    "                'pronouns': 'pronouns',\n",
    "                'origin': 'origin',\n",
    "                'race': 'race',\n",
    "                'number': 'agent_number',\n",
    "            }\n",
    "\n",
    "            # Find all data items in the info box\n",
    "            for data_item in bio_box.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "                source = data_item.get('data-source')\n",
    "                if source in bio_field_mapping:\n",
    "                    value_div = data_item.find('div', class_='pi-data-value')\n",
    "                    if value_div:\n",
    "                        bio_info[bio_field_mapping[source]] = ' '.join(value_div.stripped_strings)\n",
    "\n",
    "        #game details box\n",
    "        game_info = {}\n",
    "        if game_details_box:\n",
    "                game_field_mapping = {\n",
    "                'role': 'role',\n",
    "                'basic-abilities': 'basic',\n",
    "                'signature-abilities': 'signature',\n",
    "                'ultimate-ability': 'ultimate',        \n",
    "\n",
    "\n",
    "            }\n",
    "            # for data_item in bio_box.find_all('div', class_='pi-item pi-data pi-item-spacing pi-border-color'):\n",
    "\n",
    "        return {\n",
    "            \"agent\": name,\n",
    "            # \"role\": role,\n",
    "            \"biography\": bio_info,  # Add the biographical information\n",
    "            \"abilities\": game_info,\n",
    "            \"url\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:10<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 27 agents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    agent_links = get_agent_links()\n",
    "    \n",
    "    all_agents = []\n",
    " \n",
    "    for link in tqdm(agent_links):\n",
    "        agent_data = parse_agent_page(link)\n",
    "        if agent_data:\n",
    "            all_agents.append(agent_data)\n",
    "\n",
    "    with open(\"data/valorant_agents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_agents, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Done! Scraped {len(all_agents)} agents.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
